import streamlit as st
import pandas as pd
import json
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path

st.set_page_config(page_title="R√©sultats ML - NBA Draft 2025", page_icon="ü§ñ", layout="wide")

# Titre
st.title("ü§ñ R√©sultats du Mod√®le ML - NBA Draft 2025")
st.markdown("---")

# Charger les r√©sultats
@st.cache_data
def load_results():
    """Charge les r√©sultats du mod√®le"""
    try:
        with open('nba_draft_results.json', 'r') as f:
            results = json.load(f)
        return results
    except FileNotFoundError:
        st.error("‚ùå Fichier de r√©sultats introuvable. Ex√©cutez d'abord le mod√®le.")
        return None

@st.cache_data
def load_predictions():
    """Charge les pr√©dictions compl√®tes"""
    try:
        df = pd.read_csv('nba_draft_predictions.csv')
        return df
    except FileNotFoundError:
        st.warning("‚ö†Ô∏è Fichier de pr√©dictions introuvable.")
        return None

@st.cache_data
def load_features():
    """Charge la liste des features"""
    try:
        with open('nba_draft_features.json', 'r') as f:
            features = json.load(f)
        return features
    except FileNotFoundError:
        return None

# Charger les donn√©es
results = load_results()
predictions = load_predictions()
features_data = load_features()

if results is None:
    st.stop()

# Sidebar - Informations du mod√®le
with st.sidebar:
    st.header("‚ÑπÔ∏è Informations")
    st.metric("Version", results['model_version'])
    st.metric("Type", results['model_type'].capitalize())
    st.metric("Meilleur mod√®le", results['best_model'])
    st.metric("Joueurs analys√©s", results['n_players'])
    st.metric("Features utilis√©es", results['n_features'])
    
    st.markdown("---")
    st.markdown("### üéØ Objectif")
    st.markdown("""
    Ce mod√®le pr√©dit le **draft rank** (position 1-60) de chaque joueur 
    en se basant uniquement sur des **donn√©es observables** :
    - Stats college
    - √âvaluations scouting
    - Mesures physiques
    - Feature engineering
    """)

# Onglets principaux
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìä Vue d'ensemble",
    "üéØ Performance",
    "üìà Pr√©dictions",
    "üîç Feature Importance",
    "üìã Donn√©es d√©taill√©es"
])

# TAB 1: VUE D'ENSEMBLE
with tab1:
    st.header("üìä Vue d'ensemble des r√©sultats")
    
    col1, col2, col3, col4 = st.columns(4)
    
    perf = results['performance']
    errors = results['errors_analysis']
    
    with col1:
        st.metric(
            "MAE Ensemble",
            f"{perf['ensemble_mae']:.2f} picks",
            help="Mean Absolute Error - Erreur moyenne de pr√©diction"
        )
    
    with col2:
        st.metric(
            "RMSE",
            f"{perf['ensemble_rmse']:.2f} picks",
            help="Root Mean Squared Error - P√©nalise les grosses erreurs"
        )
    
    with col3:
        st.metric(
            "R¬≤",
            f"{perf['ensemble_r2']:.3f}",
            help="Coefficient de d√©termination - Part de variance expliqu√©e"
        )
    
    with col4:
        st.metric(
            "Spearman",
            f"{perf['ensemble_spearman']:.3f}",
            help="Corr√©lation de rang - Mesure la qualit√© du ranking"
        )
    
    st.markdown("---")
    
    # Interpr√©tation
    st.subheader("üí° Interpr√©tation")
    
    mae = perf['ensemble_mae']
    spearman = perf['ensemble_spearman']
    
    # Verdict bas√© sur MAE
    if mae < 5:
        verdict_color = "green"
        verdict_text = "üü¢ EXCELLENT"
        verdict_detail = "Pr√©dictions tr√®s pr√©cises!"
    elif mae < 8:
        verdict_color = "blue"
        verdict_text = "üü° BON"
        verdict_detail = "Pr√©dictions fiables"
    elif mae < 12:
        verdict_color = "orange"
        verdict_text = "üü† CORRECT"
        verdict_detail = "Pr√©dictions acceptables"
    else:
        verdict_color = "red"
        verdict_text = "üî¥ FAIBLE"
        verdict_detail = "Mod√®le √† am√©liorer"
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"### Performance globale: {verdict_text}")
        st.markdown(f"*{verdict_detail}*")
        st.markdown(f"""
        - **Erreur moyenne:** {mae:.1f} picks
        - **Erreur m√©diane:** {errors['median_error']:.1f} picks
        - **Erreur max:** {errors['max_error']:.1f} picks
        """)
    
    with col2:
        # Verdict bas√© sur Spearman
        if spearman > 0.7:
            st.success(f"‚úÖ **Excellente corr√©lation de rang** (Spearman = {spearman:.3f})")
            st.markdown("Le mod√®le range tr√®s bien les joueurs, m√™me si les valeurs exactes ont une marge d'erreur.")
        elif spearman > 0.5:
            st.info(f"‚úÖ **Bonne corr√©lation de rang** (Spearman = {spearman:.3f})")
        else:
            st.warning(f"‚ö†Ô∏è **Corr√©lation de rang mod√©r√©e** (Spearman = {spearman:.3f})")
    
    st.markdown("---")
    
    # Pr√©cision par tier
    st.subheader("üéØ Pr√©cision par tier de draft")
    
    tier_data = results['tier_performance']
    
    if tier_data:
        cols = st.columns(len(tier_data))
        
        for idx, (tier_name, tier_info) in enumerate(tier_data.items()):
            with cols[idx]:
                st.metric(
                    f"{tier_name}",
                    f"{tier_info['mae']:.2f} picks",
                    delta=None,
                    help=f"Picks {tier_info['range']} ({tier_info['n_players']} joueurs)"
                )
        
        st.markdown("""
        **Note:** Il est normal que les lottery picks (1-10) soient plus difficiles √† pr√©dire 
        car ils d√©pendent fortement des besoins des √©quipes et du talent "upside" difficile √† quantifier.
        """)

# TAB 2: PERFORMANCE
with tab2:
    st.header("üéØ Performance des mod√®les")
    
    # Comparaison des mod√®les
    st.subheader("üìä Comparaison des mod√®les individuels")
    
    model_names = list(perf['cv_mae_mean'].keys())
    
    comparison_data = {
        'Mod√®le': model_names,
        'CV MAE': [perf['cv_mae_mean'][name] for name in model_names],
        'CV Std': [perf['cv_mae_std'][name] for name in model_names],
        'Test MAE': [perf['test_mae'][name] for name in model_names],
        'Test RMSE': [perf['test_rmse'][name] for name in model_names],
        'R¬≤': [perf['test_r2'][name] for name in model_names],
        'Spearman': [perf['test_spearman'][name] for name in model_names],
    }
    
    df_comparison = pd.DataFrame(comparison_data)
    
    # Styliser le DataFrame
    st.dataframe(
        df_comparison.style.format({
            'CV MAE': '{:.2f}',
            'CV Std': '{:.2f}',
            'Test MAE': '{:.2f}',
            'Test RMSE': '{:.2f}',
            'R¬≤': '{:.3f}',
            'Spearman': '{:.3f}',
        }).background_gradient(subset=['Test MAE'], cmap='RdYlGn_r'),
        use_container_width=True
    )
    
    st.markdown("---")
    
    # Visualisation des performances
    col1, col2 = st.columns(2)
    
    with col1:
        # Graphique MAE par mod√®le
        fig_mae = go.Figure()
        
        fig_mae.add_trace(go.Bar(
            x=model_names,
            y=[perf['cv_mae_mean'][name] for name in model_names],
            name='CV MAE',
            marker_color='lightblue',
            error_y=dict(
                type='data',
                array=[perf['cv_mae_std'][name] for name in model_names]
            )
        ))
        
        fig_mae.add_trace(go.Bar(
            x=model_names,
            y=[perf['test_mae'][name] for name in model_names],
            name='Test MAE',
            marker_color='darkblue'
        ))
        
        fig_mae.update_layout(
            title="MAE par mod√®le",
            xaxis_title="Mod√®le",
            yaxis_title="MAE (picks)",
            barmode='group',
            height=400
        )
        
        st.plotly_chart(fig_mae, use_container_width=True)
    
    with col2:
        # Graphique Spearman par mod√®le
        fig_spearman = go.Figure()
        
        fig_spearman.add_trace(go.Bar(
            x=model_names,
            y=[perf['test_spearman'][name] for name in model_names],
            marker_color=['green' if s > 0.7 else 'orange' for s in [perf['test_spearman'][name] for name in model_names]]
        ))
        
        fig_spearman.add_hline(y=0.7, line_dash="dash", line_color="red", 
                               annotation_text="Seuil excellent (0.7)")
        
        fig_spearman.update_layout(
            title="Corr√©lation Spearman par mod√®le",
            xaxis_title="Mod√®le",
            yaxis_title="Spearman",
            height=400
        )
        
        st.plotly_chart(fig_spearman, use_container_width=True)
    
    st.markdown("---")
    
    # Poids de l'ensemble
    st.subheader("‚öñÔ∏è Poids des mod√®les dans l'ensemble")
    
    weights = results['weights']
    
    fig_weights = go.Figure(data=[
        go.Pie(
            labels=list(weights.keys()),
            values=list(weights.values()),
            hole=0.3,
            textinfo='label+percent',
            marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])
        )
    ])
    
    fig_weights.update_layout(
        title="Contribution de chaque mod√®le √† la pr√©diction finale",
        height=400
    )
    
    st.plotly_chart(fig_weights, use_container_width=True)
    
    st.info("""
    **‚ÑπÔ∏è Note:** Les poids sont calcul√©s en fonction des performances en validation crois√©e. 
    Les mod√®les les plus performants ont un poids plus √©lev√© dans la pr√©diction finale.
    """)

# TAB 3: PR√âDICTIONS
with tab3:
    st.header("üìà Analyse des pr√©dictions")
    
    if predictions is not None:
        # Statistiques sur les erreurs
        st.subheader("üìä Distribution des erreurs")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Erreur m√©diane", f"{errors['median_error']:.2f} picks")
        with col2:
            st.metric("Erreur moyenne", f"{errors['mean_error']:.2f} picks")
        with col3:
            st.metric("Top 10 accuracy", f"{errors['top10_accuracy']*100:.1f}%")
        with col4:
            st.metric("Top 5 accuracy", f"{errors['top5_accuracy']*100:.1f}%")
        
        # Graphique scatter: Pr√©dit vs R√©el
        st.subheader("üéØ Pr√©dictions vs R√©alit√©")
        
        fig_scatter = go.Figure()
        
        # Ligne parfaite (y=x)
        fig_scatter.add_trace(go.Scatter(
            x=[1, 60],
            y=[1, 60],
            mode='lines',
            name='Pr√©diction parfaite',
            line=dict(color='red', dash='dash')
        ))
        
        # Pr√©dictions
        fig_scatter.add_trace(go.Scatter(
            x=predictions['draft_rank'],
            y=predictions['predicted_rank_ensemble'],
            mode='markers',
            name='Pr√©dictions',
            marker=dict(
                size=8,
                color=predictions['prediction_error'],
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="Erreur"),
                line=dict(width=1, color='white')
            ),
            text=predictions['name'] if 'name' in predictions.columns else None,
            hovertemplate='<b>%{text}</b><br>R√©el: %{x}<br>Pr√©dit: %{y:.1f}<br>Erreur: %{marker.color:.1f}<extra></extra>'
        ))
        
        fig_scatter.update_layout(
            title="Pr√©dictions vs Draft Rank r√©el",
            xaxis_title="Draft Rank r√©el",
            yaxis_title="Draft Rank pr√©dit",
            height=600,
            hovermode='closest'
        )
        
        st.plotly_chart(fig_scatter, use_container_width=True)
        
        # Histogramme des erreurs
        st.subheader("üìä Distribution des erreurs de pr√©diction")
        
        fig_hist = go.Figure()
        
        fig_hist.add_trace(go.Histogram(
            x=predictions['prediction_error'],
            nbinsx=20,
            marker_color='lightblue',
            marker_line_color='darkblue',
            marker_line_width=1
        ))
        
        fig_hist.update_layout(
            title="Fr√©quence des erreurs",
            xaxis_title="Erreur absolue (picks)",
            yaxis_title="Nombre de joueurs",
            height=400
        )
        
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # Meilleures et pires pr√©dictions
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("‚úÖ Top 5 meilleures pr√©dictions")
            best_preds = predictions.nsmallest(5, 'prediction_error')
            
            for idx, row in best_preds.iterrows():
                with st.container():
                    st.markdown(f"""
                    **{row.get('name', f"Joueur #{idx}")}**  
                    R√©el: `{row['draft_rank']:.0f}` | Pr√©dit: `{row['predicted_rank_ensemble']:.1f}` | Erreur: `{row['prediction_error']:.1f}`
                    """)
        
        with col2:
            st.subheader("‚ùå Top 5 pires pr√©dictions")
            worst_preds = predictions.nlargest(5, 'prediction_error')
            
            for idx, row in worst_preds.iterrows():
                with st.container():
                    st.markdown(f"""
                    **{row.get('name', f"Joueur #{idx}")}**  
                    R√©el: `{row['draft_rank']:.0f}` | Pr√©dit: `{row['predicted_rank_ensemble']:.1f}` | Erreur: `{row['prediction_error']:.1f}`
                    """)

# TAB 4: FEATURE IMPORTANCE
with tab4:
    st.header("üîç Importance des features")
    
    feature_imp = pd.DataFrame(results['feature_importance'])
    
    st.subheader("üèÜ Top 20 features les plus importantes")
    
    # Graphique horizontal
    top_features = feature_imp.head(20)
    
    fig_importance = go.Figure(go.Bar(
        x=top_features['importance'],
        y=top_features['feature'],
        orientation='h',
        marker=dict(
            color=top_features['importance'],
            colorscale='Blues',
            showscale=False
        )
    ))
    
    fig_importance.update_layout(
        title="Contribution de chaque feature au mod√®le",
        xaxis_title="Importance",
        yaxis_title="Feature",
        height=600,
        yaxis=dict(autorange="reversed")
    )
    
    st.plotly_chart(fig_importance, use_container_width=True)
    
    # Analyse de la distribution
    st.markdown("---")
    st.subheader("üìä Distribution de l'importance")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Top feature", feature_imp.iloc[0]['feature'])
        st.metric("Importance", f"{feature_imp.iloc[0]['importance']:.1%}")
        
        # V√©rifier si dominante
        if feature_imp.iloc[0]['importance'] > 0.5:
            st.error("‚ö†Ô∏è Feature ultra-dominante (>50%) - Risque de surapprentissage")
        elif feature_imp.iloc[0]['importance'] > 0.3:
            st.warning("‚ö†Ô∏è Feature tr√®s dominante (>30%)")
        else:
            st.success("‚úÖ Distribution √©quilibr√©e")
    
    with col2:
        # Top 3 features cumul√©es
        top3_cumul = feature_imp.head(3)['importance'].sum()
        st.metric("Top 3 features (cumul√©)", f"{top3_cumul:.1%}")
        
        # Top 10 cumul√©
        top10_cumul = feature_imp.head(10)['importance'].sum()
        st.metric("Top 10 features (cumul√©)", f"{top10_cumul:.1%}")
    
    # Table compl√®te
    st.markdown("---")
    st.subheader("üìã Table compl√®te des features")
    
    st.dataframe(
        feature_imp.style.format({'importance': '{:.4f}'})
        .background_gradient(subset=['importance'], cmap='Blues'),
        use_container_width=True,
        height=400
    )

# TAB 5: DONN√âES D√âTAILL√âES
with tab5:
    st.header("üìã Donn√©es d√©taill√©es")
    
    if predictions is not None:
        st.subheader("üéØ Pr√©dictions compl√®tes")
        
        # Options de filtrage
        col1, col2, col3 = st.columns(3)
        
        with col1:
            tier_filter = st.selectbox(
                "Filtrer par tier",
                ["Tous", "Lottery (1-10)", "First Round (11-30)", "Second Round (31-60)"]
            )
        
        with col2:
            error_threshold = st.slider(
                "Erreur max (picks)",
                0, int(predictions['prediction_error'].max()) + 1,
                int(predictions['prediction_error'].max()) + 1
            )
        
        with col3:
            sort_by = st.selectbox(
                "Trier par",
                ["Draft Rank", "Erreur (d√©croissant)", "Erreur (croissant)"]
            )
        
        # Appliquer les filtres
        filtered_df = predictions.copy()
        
        if tier_filter == "Lottery (1-10)":
            filtered_df = filtered_df[filtered_df['draft_rank'] <= 10]
        elif tier_filter == "First Round (11-30)":
            filtered_df = filtered_df[(filtered_df['draft_rank'] >= 11) & (filtered_df['draft_rank'] <= 30)]
        elif tier_filter == "Second Round (31-60)":
            filtered_df = filtered_df[filtered_df['draft_rank'] >= 31]
        
        filtered_df = filtered_df[filtered_df['prediction_error'] <= error_threshold]
        
        if sort_by == "Draft Rank":
            filtered_df = filtered_df.sort_values('draft_rank')
        elif sort_by == "Erreur (d√©croissant)":
            filtered_df = filtered_df.sort_values('prediction_error', ascending=False)
        else:
            filtered_df = filtered_df.sort_values('prediction_error')
        
        st.dataframe(
            filtered_df.style.format({
                'draft_rank': '{:.0f}',
                'predicted_rank_ensemble': '{:.1f}',
                'prediction_error': '{:.1f}'
            }).background_gradient(subset=['prediction_error'], cmap='RdYlGn_r'),
            use_container_width=True,
            height=600
        )
        
        # T√©l√©chargement
        st.download_button(
            label="üì• T√©l√©charger les pr√©dictions (CSV)",
            data=filtered_df.to_csv(index=False).encode('utf-8'),
            file_name='nba_draft_predictions_filtered.csv',
            mime='text/csv'
        )
    
    # Liste des features utilis√©es
    if features_data:
        st.markdown("---")
        st.subheader("üìù Features utilis√©es dans le mod√®le")
        
        st.info(f"**{features_data['n_features']} features** au total")
        
        # Afficher en colonnes
        n_cols = 3
        cols = st.columns(n_cols)
        
        for idx, feature in enumerate(features_data['features']):
            with cols[idx % n_cols]:
                st.markdown(f"‚Ä¢ `{feature}`")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>üèÄ NBA Draft 2025 - Mod√®le de Machine Learning v3 (Clean)</p>
    <p style='font-size: 0.8em; color: gray;'>
        Bas√© uniquement sur des donn√©es observables ‚Ä¢ Sans data leakage ‚Ä¢ 
        Features: Stats college + Scouting + Physique + Feature Engineering
    </p>
</div>
""", unsafe_allow_html=True)