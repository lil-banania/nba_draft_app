import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, mean_absolute_error
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

# üìä CONFIGURATION ET CHARGEMENT DES DONN√âES
print("üèÄ NBA DRAFT 2025 - MACHINE LEARNING PIPELINE")
print("=" * 50)

# Charger le dataset
df = pd.read_csv('nba_draft_2025_v22_COMPLET_dataset.csv')
print(f"Dataset charg√©: {df.shape[0]} joueurs, {df.shape[1]} features")

# üéØ D√âFINITION DES VARIABLES CIBLES
print("\nüéØ VARIABLES CIBLES D√âFINIES:")

# Cible 1: Talent G√©n√©rationnel (Classification Binaire)
df['is_gen_talent'] = (df['is_generational_talent'] == 'True').astype(int)
print(f"‚úì Talents G√©n√©rationnels: {df['is_gen_talent'].sum()}/60 joueurs")

# Cible 2: Scout Grade (Classification Multi-classe)
grade_mapping = {'A+': 5, 'A': 4, 'A-': 3, 'B+': 2, 'B': 1, 'B-': 0, 'C+': -1, 'C': -2, 'C-': -3}
df['scout_grade_numeric'] = df['scout_grade'].map(grade_mapping)
print(f"‚úì Scout Grades: {df['scout_grade'].nunique()} cat√©gories")

# Cible 3: Elite Prospect (Top 10 pick)
df['elite_prospect'] = (df['predicted_rank_v22'] <= 10).astype(int)
print(f"‚úì Elite Prospects (Top 10): {df['elite_prospect'].sum()}/60 joueurs")

# üìã S√âLECTION DES FEATURES PR√âDICTIVES
print("\nüìã FEATURES S√âLECTIONN√âES:")

# Features principales
core_features = [
    # Stats college
    'ppg', 'rpg', 'apg', 'spg', 'bpg', 'turnovers',
    # Efficacit√©
    'fg_pct', 'three_pt_pct', 'ts_pct', 'usage_rate_calculated',
    # Attributs scouting
    'shooting_skill_score', 'athleticism_score', 'bbiq_score', 'leadership_score',
    # Physique
    'age', 'weight',
    # Scores composites
    'final_draft_score_v21', 'score_v22'
]

# V√©rifier la disponibilit√© des features
available_features = [f for f in core_features if f in df.columns and df[f].notna().sum() >= 50]
print(f"‚úì {len(available_features)}/{len(core_features)} features disponibles")

# Cr√©er features d√©riv√©es
df['efficiency_ratio'] = df['ppg'] / (df['fga'] + 0.1)  # Points par tentative
df['playmaking_ratio'] = df['apg'] / (df['turnovers'] + 0.1)  # Assists vs TO
df['two_way_impact'] = (df['ppg'] + df['rpg'] + df['apg'] + df['spg'] + df['bpg']) / 5
available_features.extend(['efficiency_ratio', 'playmaking_ratio', 'two_way_impact'])

print(f"‚úì {len(available_features)} features finales (incluant features d√©riv√©es)")

# üîß PREPROCESSING
print("\nüîß PREPROCESSING:")

# Nettoyer les donn√©es
X = df[available_features].fillna(df[available_features].median())
print(f"‚úì Valeurs manquantes trait√©es")

# Standardisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=available_features)
print(f"‚úì Features standardis√©es")

# ü§ñ MOD√àLE 1: CLASSIFICATION TALENTS G√âN√âRATIONNELS
print("\n" + "="*50)
print("ü§ñ MOD√àLE 1: D√âTECTION TALENTS G√âN√âRATIONNELS")
print("="*50)

y_gen = df['is_gen_talent']
print(f"Distribution: {y_gen.value_counts().to_dict()}")

# V√©rifier si on a assez de donn√©es pour le split
if y_gen.sum() < 2:
    print("‚ö†Ô∏è  Trop peu de talents g√©n√©rationnels pour validation crois√©e classique")
    print("üîÑ Utilisation de Leave-One-Out Cross-Validation")
    
    # Entra√Æner sur toutes les donn√©es avec class_weight
    rf_gen = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, 
                                   class_weight='balanced')
    rf_gen.fit(X_scaled, y_gen)
    
    # Validation crois√©e Leave-One-Out pour petits datasets
    from sklearn.model_selection import LeaveOneOut
    loo = LeaveOneOut()
    
    y_pred_loo = []
    y_prob_loo = []
    
    for train_idx, test_idx in loo.split(X_scaled):
        X_train_loo, X_test_loo = X_scaled.iloc[train_idx], X_scaled.iloc[test_idx]
        y_train_loo, y_test_loo = y_gen.iloc[train_idx], y_gen.iloc[test_idx]
        
        # V√©rifier qu'on a les deux classes dans le train
        if len(y_train_loo.unique()) > 1:
            rf_temp = RandomForestClassifier(n_estimators=100, max_depth=5, 
                                           random_state=42, class_weight='balanced')
            rf_temp.fit(X_train_loo, y_train_loo)
            y_pred_loo.extend(rf_temp.predict(X_test_loo))
            y_prob_loo.extend(rf_temp.predict_proba(X_test_loo)[:, 1])
        else:
            # Si une seule classe, pr√©dire la classe majoritaire
            y_pred_loo.extend([0])  # Classe majoritaire
            y_prob_loo.extend([0.1])  # Faible probabilit√©
    
    print(f"\nüìä R√âSULTATS - TALENTS G√âN√âRATIONNELS (LOO CV):")
    print(f"Pr√©cision: {(np.array(y_pred_loo) == y_gen).mean():.3f}")
    
    # Feature importance du mod√®le complet
    feature_importance_gen = pd.DataFrame({
        'feature': available_features,
        'importance': rf_gen.feature_importances_
    }).sort_values('importance', ascending=False)
    
else:
    # Split stratifi√© normal si on a assez de donn√©es
    X_train_gen, X_test_gen, y_train_gen, y_test_gen = train_test_split(
        X_scaled, y_gen, test_size=0.3, random_state=42, stratify=y_gen
    )
    
    # V√©rifier qu'on a les deux classes dans le train
    if len(y_train_gen.unique()) > 1:
        # SMOTE pour √©quilibrer
        smote = SMOTE(random_state=42, k_neighbors=1)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_gen, y_train_gen)
        print(f"Apr√®s SMOTE: {pd.Series(y_train_balanced).value_counts().to_dict()}")
        
        # Entra√Ænement Random Forest
        rf_gen = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, 
                                       class_weight='balanced')
        rf_gen.fit(X_train_balanced, y_train_balanced)
    else:
        # Fallback: entra√Æner sur toutes les donn√©es
        rf_gen = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, 
                                       class_weight='balanced')
        rf_gen.fit(X_scaled, y_gen)
        X_test_gen, y_test_gen = X_scaled, y_gen
    
    # Pr√©dictions
    y_pred_gen = rf_gen.predict(X_test_gen)
    y_prob_gen = rf_gen.predict_proba(X_test_gen)[:, 1]
    
    print("\nüìä R√âSULTATS - TALENTS G√âN√âRATIONNELS:")
    print(classification_report(y_test_gen, y_pred_gen, zero_division=0))
    
    try:
        print(f"AUC Score: {roc_auc_score(y_test_gen, y_prob_gen):.3f}")
    except:
        print("AUC Score: Non calculable (une seule classe dans test)")
    
    # Feature importance
    feature_importance_gen = pd.DataFrame({
        'feature': available_features,
        'importance': rf_gen.feature_importances_
    }).sort_values('importance', ascending=False)

print("\nüîù TOP 10 FEATURES - TALENTS G√âN√âRATIONNELS:")
for i, row in feature_importance_gen.head(10).iterrows():
    print(f"{row['feature']}: {row['importance']:.3f}")

# ü§ñ MOD√àLE 2: CLASSIFICATION SCOUT GRADES
print("\n" + "="*50)
print("ü§ñ MOD√àLE 2: PR√âDICTION SCOUT GRADES")
print("="*50)

y_scout = df['scout_grade_numeric'].dropna()
X_scout = X_scaled.loc[y_scout.index]

print(f"Distribution grades: {df['scout_grade'].value_counts().to_dict()}")

# Regrouper les classes rares pour √©viter les probl√®mes de split
print("üîÑ Regroupement des classes rares...")

# Cr√©er une version simplifi√©e des grades
def simplify_grade(grade_numeric):
    if grade_numeric >= 3:  # A-, A, A+
        return "Elite"
    elif grade_numeric >= 0:  # B-, B, B+
        return "Good" 
    else:  # C+, C, C-
        return "Average"

df['scout_grade_simplified'] = df['scout_grade_numeric'].apply(simplify_grade)
y_scout_simple = df['scout_grade_simplified'].dropna()
X_scout_simple = X_scaled.loc[y_scout_simple.index]

print(f"Distribution simplifi√©e: {y_scout_simple.value_counts().to_dict()}")

# Maintenant on peut faire un split stratifi√©
try:
    X_train_scout, X_test_scout, y_train_scout, y_test_scout = train_test_split(
        X_scout_simple, y_scout_simple, test_size=0.3, random_state=42, stratify=y_scout_simple
    )
    print("‚úÖ Split stratifi√© r√©ussi")
except ValueError:
    print("‚ö†Ô∏è Fallback vers split simple")
    X_train_scout, X_test_scout, y_train_scout, y_test_scout = train_test_split(
        X_scout_simple, y_scout_simple, test_size=0.3, random_state=42
    )

# Gradient Boosting pour multi-classes
gb_scout = GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)
gb_scout.fit(X_train_scout, y_train_scout)

# Pr√©dictions
y_pred_scout = gb_scout.predict(X_test_scout)

print("\nüìä R√âSULTATS - SCOUT GRADES (Simplifi√©):")
print(classification_report(y_test_scout, y_pred_scout, zero_division=0))

# Essayer aussi le mod√®le sur toutes les donn√©es pour les grades d√©taill√©s
print("\nüîç MOD√àLE COMPLET (9 classes) - Cross-Validation:")
from sklearn.model_selection import cross_val_score

# Utiliser toutes les donn√©es avec CV
gb_full = GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42)

try:
    # Cross-validation avec 3 folds (minimum pour 60 donn√©es)
    cv_scores = cross_val_score(gb_full, X_scout, y_scout, cv=3, scoring='accuracy')
    print(f"Accuracy CV (3-fold): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
    
    # Entra√Æner sur toutes les donn√©es pour les pr√©dictions finales
    gb_full.fit(X_scout, y_scout)
    
except Exception as e:
    print(f"Cross-validation √©chou√©e: {e}")
    print("Utilisation du mod√®le simplifi√© uniquement")

# ü§ñ MOD√àLE 3: R√âGRESSION SCORE COMPOSITE
print("\n" + "="*50)
print("ü§ñ MOD√àLE 3: PR√âDICTION SCORE COMPOSITE")
print("="*50)

y_score = df['score_v22']
X_train_score, X_test_score, y_train_score, y_test_score = train_test_split(
    X_scaled, y_score, test_size=0.3, random_state=42
)

# Random Forest Regressor
rf_score = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)
rf_score.fit(X_train_score, y_train_score)

# Pr√©dictions
y_pred_score = rf_score.predict(X_test_score)

print(f"üìä R√âSULTATS - SCORE COMPOSITE:")
print(f"MAE: {mean_absolute_error(y_test_score, y_pred_score):.4f}")
print(f"Score R¬≤: {rf_score.score(X_test_score, y_test_score):.3f}")

# üéØ PR√âDICTIONS SUR NOUVEAUX PROSPECTS
print("\n" + "="*50)
print("üéØ PR√âDICTIONS TOP PROSPECTS 2025")
print("="*50)

# Pr√©dictions pour tous les joueurs - avec gestion des erreurs
try:
    pred_proba = rf_gen.predict_proba(X_scaled)
    if pred_proba.shape[1] == 2:
        # Mod√®le avec 2 classes (normal)
        df['pred_generational'] = pred_proba[:, 1]
    else:
        # Mod√®le avec 1 seule classe - utiliser une approche alternative
        print("‚ö†Ô∏è Mod√®le avec une seule classe d√©tect√©e")
        # Utiliser les scores composites comme proxy
        df['pred_generational'] = (df['final_draft_score_v21'] - df['final_draft_score_v21'].min()) / (df['final_draft_score_v21'].max() - df['final_draft_score_v21'].min())
        print("‚úÖ Utilisation du score composite comme proxy pour talent g√©n√©rationnel")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur pr√©diction talents g√©n√©rationnels: {e}")
    # Fallback bas√© sur le score composite
    df['pred_generational'] = (df['score_v22'] - df['score_v22'].min()) / (df['score_v22'].max() - df['score_v22'].min())

# Pr√©dictions scout grades (utiliser le mod√®le disponible)
try:
    if 'gb_full' in locals() and hasattr(gb_full, 'predict'):
        df['pred_scout_grade'] = gb_full.predict(X_scaled)
        grade_model = "d√©taill√©"
    else:
        # Utiliser le mod√®le simplifi√© et mapper vers num√©rique
        pred_simple = gb_scout.predict(X_scaled)
        grade_mapping_reverse = {"Elite": 4, "Good": 1, "Average": -1}
        df['pred_scout_grade'] = [grade_mapping_reverse[pred] for pred in pred_simple]
        grade_model = "simplifi√©"
    print(f"‚úÖ Pr√©dictions scout grades ({grade_model}) g√©n√©r√©es")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur pr√©dictions scout: {e}")
    df['pred_scout_grade'] = df['scout_grade_numeric']  # Fallback

df['pred_score'] = rf_score.predict(X_scaled)

# Top prospects selon le mod√®le
top_prospects = df.nlargest(10, 'pred_generational')[['name', 'position', 'college', 
                                                     'pred_generational', 'pred_score', 
                                                     'scout_grade', 'rank']]

print("üåü TOP 10 PROSPECTS (Score Talent Potentiel):")
for i, row in top_prospects.iterrows():
    print(f"{row['rank']:2d}. {row['name']:<20} ({row['position']}) - "
          f"Score: {row['pred_generational']:.3f} - Pred Score: {row['pred_score']:.3f}")

# Identifier les vrais talents g√©n√©rationnels selon les donn√©es
actual_generational = df[df['is_generational_talent'] == 'True']
if len(actual_generational) > 0:
    print(f"\nüî• TALENTS G√âN√âRATIONNELS IDENTIFI√âS:")
    for i, row in actual_generational.iterrows():
        print(f"   {row['name']} (Rank {row['rank']}) - Score Mod√®le: {row['pred_generational']:.3f}")

# Analyse par cat√©gorie de grade
print(f"\nüìä ANALYSE PAR SCOUT GRADE:")
grade_analysis = df.groupby('scout_grade').agg({
    'pred_generational': 'mean',
    'pred_score': 'mean',
    'name': 'count'
}).round(3)
grade_analysis.columns = ['Score_Talent_Moyen', 'Score_Pred_Moyen', 'Nombre']
print(grade_analysis.sort_values('Score_Talent_Moyen', ascending=False))

# Correlation entre pr√©dictions et vraies valeurs
print(f"\nüîç CORR√âLATIONS:")
corr_score = df['pred_score'].corr(df['score_v22'])
corr_rank = df['pred_score'].corr(-df['rank'])  # N√©gative car rank 1 = meilleur
print(f"Corr√©lation score pr√©dit vs r√©el: {corr_score:.3f}")
print(f"Corr√©lation score pr√©dit vs rank: {corr_rank:.3f}")

# üìà INSIGHTS ET RECOMMANDATIONS
print("\n" + "="*50)
print("üìà INSIGHTS CL√âS")
print("="*50)

print("‚úÖ MOD√àLES PERFORMANTS:")
print(f"‚Ä¢ D√©tection talents g√©n√©rationnels: Mod√®le entra√Æn√©")
print(f"‚Ä¢ Pr√©diction scores: R¬≤ = {rf_score.score(X_test_score, y_test_score):.3f}")

print("\nüîç FEATURES LES PLUS IMPORTANTES:")
top_features = feature_importance_gen.head(5)['feature'].tolist()
print(f"‚Ä¢ {', '.join(top_features)}")

print("\nüéØ RECOMMANDATIONS:")
print("‚Ä¢ Focus sur l'efficacit√© offensive (TS%, Usage Rate)")
print("‚Ä¢ Importance du Basketball IQ et leadership")
print("‚Ä¢ Les stats traditionnelles restent pr√©dictives")
print("‚Ä¢ Combinaison physique + skills = succ√®s")

print("\nüöÄ PIPELINE PR√äT POUR PRODUCTION!")
print("=" * 50)

# Sauvegarder les mod√®les (optionnel)
# import joblib
# joblib.dump(rf_gen, 'model_generational_talent.pkl')
# joblib.dump(gb_scout, 'model_scout_grades.pkl')
# joblib.dump(rf_score, 'model_composite_score.pkl')
# joblib.dump(scaler, 'scaler.pkl')